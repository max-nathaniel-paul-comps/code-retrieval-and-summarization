{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"../../data/iyer/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_contents = train_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in train_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        train.append(line.split('\\t')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52795"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "from text_data_utils import *\n",
    "from os import path\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = tokenize_texts(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.Word2Vec(train_tok).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(text) for text in train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = tokenized_texts_to_tensor(train_tok, wv, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.507817  ,  0.1484023 , -0.20681311,  0.62617654,  0.5449142 ,\n",
       "       -0.5449318 , -0.21301612,  1.7924953 ,  0.22406252, -0.61348665,\n",
       "        0.24831563, -1.0640328 ,  0.36666486,  0.04502412,  0.4453507 ,\n",
       "       -0.5475464 , -0.26597756, -0.48045406,  0.45849183, -0.46563247,\n",
       "       -0.05305701,  0.5125075 , -0.33755347,  1.2249124 ,  0.28303462,\n",
       "       -0.70756435, -0.57670695,  0.6581089 ,  0.5804113 , -0.70469946,\n",
       "       -0.04943128, -1.0484043 ,  0.67638755,  0.88557476,  0.3741503 ,\n",
       "       -1.1953326 ,  1.3866937 , -0.8176652 ,  0.3666112 ,  0.9233758 ,\n",
       "        0.21730971,  0.49158496, -0.8107285 ,  0.1139212 ,  0.79562056,\n",
       "        1.1318586 ,  0.29344016, -0.7276022 ,  0.2629111 ,  0.63989407,\n",
       "       -0.35141924,  0.14826865, -0.29793808,  0.9161196 ,  0.3829815 ,\n",
       "        0.66493315,  0.39652708, -0.771028  , -0.43014258, -0.270518  ,\n",
       "       -0.316743  ,  0.887334  ,  0.34691614, -0.3814799 ,  0.6163675 ,\n",
       "       -1.131697  , -0.51892763, -0.00366329, -0.26000375,  0.11675325,\n",
       "       -0.84780043, -0.02043382, -0.19194306,  0.67947334, -0.73192495,\n",
       "        0.951115  , -1.1450218 ,  0.628151  , -1.5701705 , -1.4437927 ,\n",
       "       -0.44128573, -0.63668656,  0.2793874 ,  0.89433473,  0.15539338,\n",
       "       -0.39474675, -0.22368136, -0.42391005, -0.3979404 , -0.68721074,\n",
       "        0.2100048 ,  0.41150558, -0.6811153 , -0.37361598,  0.24823962,\n",
       "        1.8220862 ,  0.15637091,  0.23072796,  0.26384035, -0.04945431],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_fl = np.reshape(train_tensor, (train_tensor.shape[0], train_tensor.shape[1] * train_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52795, 3900)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor_fl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_contents = open('../../data/iyer/valid.txt').readlines()\n",
    "val = []\n",
    "for line in val_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        val.append(line.split('\\t')[2])\n",
    "val_tok = tokenize_texts(val)\n",
    "val_tensor = tokenized_texts_to_tensor(val_tok, wv, max_len)\n",
    "val_tensor_fl = np.reshape(val_tensor, (val_tensor.shape[0], val_tensor.shape[1] * val_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_vae import MLPVariationalAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor_fl.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 25 completed, training loss = 0.14636781811714172, validation loss = 0.0832347571849823\n",
      "Epoch 2 of 25 completed, training loss = 0.07747786492109299, validation loss = 0.06885802000761032\n",
      "Epoch 3 of 25 completed, training loss = 0.064786896109581, validation loss = 0.05959059298038483\n",
      "Epoch 4 of 25 completed, training loss = 0.055100101977586746, validation loss = 0.05074354633688927\n",
      "Epoch 5 of 25 completed, training loss = 0.04726957157254219, validation loss = 0.04426702857017517\n",
      "Epoch 6 of 25 completed, training loss = 0.041635360568761826, validation loss = 0.039497680962085724\n",
      "Epoch 7 of 25 completed, training loss = 0.03742150589823723, validation loss = 0.03579166531562805\n",
      "Epoch 8 of 25 completed, training loss = 0.03415169194340706, validation loss = 0.03292384371161461\n",
      "Epoch 9 of 25 completed, training loss = 0.0315316803753376, validation loss = 0.030508067458868027\n",
      "Epoch 10 of 25 completed, training loss = 0.02935452200472355, validation loss = 0.028581973165273666\n",
      "Epoch 11 of 25 completed, training loss = 0.027520932257175446, validation loss = 0.02695556730031967\n",
      "Epoch 12 of 25 completed, training loss = 0.025952158495783806, validation loss = 0.025571268051862717\n",
      "Epoch 13 of 25 completed, training loss = 0.024601878598332405, validation loss = 0.024362269788980484\n",
      "Epoch 14 of 25 completed, training loss = 0.02342378906905651, validation loss = 0.02332312986254692\n",
      "Epoch 15 of 25 completed, training loss = 0.022368954494595528, validation loss = 0.022431764751672745\n",
      "Epoch 16 of 25 completed, training loss = 0.021445170044898987, validation loss = 0.021626446396112442\n",
      "Epoch 17 of 25 completed, training loss = 0.020617665722966194, validation loss = 0.02089899405837059\n",
      "Epoch 18 of 25 completed, training loss = 0.019889404997229576, validation loss = 0.020289257168769836\n",
      "Epoch 19 of 25 completed, training loss = 0.01926255412399769, validation loss = 0.019754167646169662\n",
      "Epoch 20 of 25 completed, training loss = 0.018738994374871254, validation loss = 0.01929483190178871\n",
      "Epoch 21 of 25 completed, training loss = 0.018372029066085815, validation loss = 0.018864277750253677\n",
      "Epoch 22 of 25 completed, training loss = 0.018230507150292397, validation loss = 0.018377767875790596\n",
      "Epoch 23 of 25 completed, training loss = 0.01858494244515896, validation loss = 0.01814064010977745\n",
      "Epoch 24 of 25 completed, training loss = 0.020300550386309624, validation loss = 0.019563598558306694\n",
      "Epoch 25 of 25 completed, training loss = 0.021274054422974586, validation loss = 0.024607986211776733\n"
     ]
    }
   ],
   "source": [
    "model = MLPVariationalAutoEncoder(train_tensor_fl.shape[1], latent_dim, final_activation=None)\n",
    "model.train(train_tensor_fl, val_tensor_fl, 25, 512, tf.keras.optimizers.Adam(learning_rate=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random_idx = random.randrange(train_tensor.shape[0])\n",
    "inp = np.array([train_tensor[random_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Set) Input:  ['<s>', 'How', 'can', 'I', 'read', 'the', 'properties', 'of', 'a', 'C', '#', 'class', 'dynamically', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Training Set) Input: \", tensor_to_tokenized_texts(inp, wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = np.reshape(model.decode(model.encode(np.array([train_tensor_fl[random_idx]])).sample()), (1, train_tensor.shape[1], train_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Set) Reconstructed:  ['<s>', 'How', 'can', 'I', 'read', 'the', 'properties', 'of', 'a', 'C', '#', 'class', 'dynamically', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Training Set) Reconstructed: \", tensor_to_tokenized_texts(rec, wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3962078 ,  0.12766938, -0.2304027 ,  0.6789929 ,  0.55895853,\n",
       "       -0.57951427, -0.20617224,  1.7013891 ,  0.23728736, -0.61293125,\n",
       "        0.20292214, -1.0698476 ,  0.3878635 ,  0.03705277,  0.45512006,\n",
       "       -0.5272546 , -0.31592438, -0.5149551 ,  0.5133568 , -0.461101  ,\n",
       "       -0.04720134,  0.47412825, -0.33917975,  1.101306  ,  0.26600316,\n",
       "       -0.6783844 , -0.5514709 ,  0.66507995,  0.58273125, -0.7052709 ,\n",
       "       -0.00686422, -1.0469769 ,  0.62901074,  0.8767672 ,  0.3506065 ,\n",
       "       -1.1282785 ,  1.3190396 , -0.7990406 ,  0.2966875 ,  0.90850127,\n",
       "        0.1168989 ,  0.45771733, -0.809105  ,  0.0999707 ,  0.7554718 ,\n",
       "        1.0906658 ,  0.3132154 , -0.6970688 ,  0.29668817,  0.5814272 ,\n",
       "       -0.35530728,  0.14546826, -0.2444585 ,  0.93830204,  0.42566013,\n",
       "        0.68332213,  0.40024167, -0.79780513, -0.47994983, -0.28567576,\n",
       "       -0.27960265,  0.89332455,  0.34961796, -0.33336848,  0.6311755 ,\n",
       "       -0.98190343, -0.509345  , -0.0158149 , -0.2633227 ,  0.121819  ,\n",
       "       -0.88602024, -0.00769808, -0.20050302,  0.6497827 , -0.7156183 ,\n",
       "        0.987248  , -1.1537135 ,  0.6166962 , -1.5191934 , -1.357088  ,\n",
       "       -0.4844161 , -0.59597254,  0.3195625 ,  0.9079555 ,  0.17906873,\n",
       "       -0.42543486, -0.22955205, -0.42772213, -0.42667335, -0.5825085 ,\n",
       "        0.17479208,  0.35024843, -0.649006  , -0.33099848,  0.21105227,\n",
       "        1.8027225 ,  0.14861777,  0.24994057,  0.25755462, -0.03324008],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.507817  ,  0.1484023 , -0.20681311,  0.62617654,  0.5449142 ,\n",
       "       -0.5449318 , -0.21301612,  1.7924953 ,  0.22406252, -0.61348665,\n",
       "        0.24831563, -1.0640328 ,  0.36666486,  0.04502412,  0.4453507 ,\n",
       "       -0.5475464 , -0.26597756, -0.48045406,  0.45849183, -0.46563247,\n",
       "       -0.05305701,  0.5125075 , -0.33755347,  1.2249124 ,  0.28303462,\n",
       "       -0.70756435, -0.57670695,  0.6581089 ,  0.5804113 , -0.70469946,\n",
       "       -0.04943128, -1.0484043 ,  0.67638755,  0.88557476,  0.3741503 ,\n",
       "       -1.1953326 ,  1.3866937 , -0.8176652 ,  0.3666112 ,  0.9233758 ,\n",
       "        0.21730971,  0.49158496, -0.8107285 ,  0.1139212 ,  0.79562056,\n",
       "        1.1318586 ,  0.29344016, -0.7276022 ,  0.2629111 ,  0.63989407,\n",
       "       -0.35141924,  0.14826865, -0.29793808,  0.9161196 ,  0.3829815 ,\n",
       "        0.66493315,  0.39652708, -0.771028  , -0.43014258, -0.270518  ,\n",
       "       -0.316743  ,  0.887334  ,  0.34691614, -0.3814799 ,  0.6163675 ,\n",
       "       -1.131697  , -0.51892763, -0.00366329, -0.26000375,  0.11675325,\n",
       "       -0.84780043, -0.02043382, -0.19194306,  0.67947334, -0.73192495,\n",
       "        0.951115  , -1.1450218 ,  0.628151  , -1.5701705 , -1.4437927 ,\n",
       "       -0.44128573, -0.63668656,  0.2793874 ,  0.89433473,  0.15539338,\n",
       "       -0.39474675, -0.22368136, -0.42391005, -0.3979404 , -0.68721074,\n",
       "        0.2100048 ,  0.41150558, -0.6811153 , -0.37361598,  0.24823962,\n",
       "        1.8220862 ,  0.15637091,  0.23072796,  0.26384035, -0.04945431],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_contents = open('../../data/iyer/test.txt').readlines()\n",
    "test = []\n",
    "for line in test_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        test.append(line.split('\\t')[2])\n",
    "test_tok = tokenize_texts(test)\n",
    "test_tensor = tokenized_texts_to_tensor(test_tok, wv, max_len)\n",
    "test_tensor_fl = np.reshape(test_tensor, (test_tensor.shape[0], test_tensor.shape[1] * test_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random_idx = random.randrange(test_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Input:  ['<s>', 'help', 'with', 'a', 'two', 'table', 'linq', 'query', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Input: \", tensor_to_tokenized_texts(np.array([test_tensor[random_idx]]), wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Reconstructed:  ['<s>', 'help', 'with', 'a', 'two', 'table', 'linq', 'query', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Reconstructed: \", tensor_to_tokenized_texts(np.reshape(model.decode(model.encode(np.array([test_tensor_fl[random_idx]])).sample()), (1, test_tensor.shape[1], test_tensor.shape[2])), wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
