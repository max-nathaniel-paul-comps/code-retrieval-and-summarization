{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"../../data/iyer/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_contents = train_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in train_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        train.append(line.split('\\t')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52795"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "from text_data_utils import *\n",
    "from os import path\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = tokenize_texts(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.Word2Vec(train_tok).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(text) for text in train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = tokenized_texts_to_tensor(train_tok, wv, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.40965456e-01, -1.68591738e-01, -1.21166356e-01, -7.00770020e-02,\n",
       "        5.42261541e-01, -1.90423921e-01, -2.93175071e-01, -3.36992919e-01,\n",
       "       -4.43560123e-01, -8.06656405e-02,  9.69258785e-01,  5.79911768e-01,\n",
       "        3.59580606e-01,  1.58664152e-01, -7.05792129e-01,  4.06532168e-01,\n",
       "       -6.12411857e-01,  1.09886892e-01,  2.16908717e+00, -2.16471687e-01,\n",
       "       -2.39513546e-01,  9.52519476e-01, -5.05990721e-02,  8.70549202e-01,\n",
       "       -9.07646000e-01, -5.23253977e-02, -6.07500911e-01,  1.13252950e+00,\n",
       "        9.25281227e-01,  5.35802916e-02, -2.36804366e-01, -9.57369626e-01,\n",
       "        4.25008565e-01,  7.79695153e-01, -4.74927723e-01,  5.10380507e-01,\n",
       "        2.22434029e-01, -7.18683600e-02, -1.13020885e+00,  8.24752092e-01,\n",
       "       -6.55958876e-02, -2.18344495e-01, -4.31710213e-01, -5.30911982e-01,\n",
       "        3.63499045e-01, -9.79624540e-02, -1.78358674e+00,  9.40221012e-01,\n",
       "       -4.70870584e-01,  1.10352505e-02,  1.74193934e-01,  5.21314383e-01,\n",
       "       -4.24552917e-01, -9.41051602e-01,  2.30740458e-01, -1.93403177e-02,\n",
       "        7.78161347e-01, -1.88773304e-01,  2.51424946e-02, -2.52789378e-01,\n",
       "       -5.79085588e-01,  2.35751972e-01,  3.50313097e-01,  2.79037714e-01,\n",
       "        1.11905622e+00, -3.22357446e-01, -2.74088115e-01,  3.26640815e-01,\n",
       "       -5.14982402e-01,  3.89300495e-01, -1.39183366e+00,  5.94153166e-01,\n",
       "       -1.53639819e-02,  1.29003194e-03, -4.92095798e-02,  3.70644540e-01,\n",
       "        5.35996966e-02,  1.70886338e-01, -3.46060574e-01, -8.57908666e-01,\n",
       "       -5.73585033e-01, -7.06369519e-01, -1.90288737e-01, -3.27025115e-01,\n",
       "        8.41354609e-01, -8.43108445e-02, -5.21707594e-01,  4.36977893e-02,\n",
       "        4.87568527e-01, -3.54488492e-02, -1.05696893e+00, -5.14669120e-01,\n",
       "        1.09216237e+00,  2.75841534e-01, -1.92883158e+00,  8.94945264e-01,\n",
       "        8.10080707e-01,  1.49253130e+00,  2.95331359e-01,  9.55897212e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_fl = np.reshape(train_tensor, (train_tensor.shape[0], train_tensor.shape[1] * train_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52795, 39, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52795, 3900)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor_fl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_contents = open('../../data/iyer/valid.txt').readlines()\n",
    "val = []\n",
    "for line in val_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        val.append(line.split('\\t')[2])\n",
    "val_tok = tokenize_texts(val)\n",
    "val_tensor = tokenized_texts_to_tensor(val_tok, wv, max_len)\n",
    "val_tensor_fl = np.reshape(val_tensor, (val_tensor.shape[0], val_tensor.shape[1] * val_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_vae import MLPVariationalAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52795 samples, validate on 6599 samples\n",
      "Epoch 1/25\n",
      "52795/52795 [==============================] - 8s 152us/sample - loss: 0.0893 - val_loss: 0.0599\n",
      "Epoch 2/25\n",
      "52795/52795 [==============================] - 5s 101us/sample - loss: 0.0526 - val_loss: 0.0473\n",
      "Epoch 3/25\n",
      "52795/52795 [==============================] - 5s 104us/sample - loss: 0.0428 - val_loss: 0.0413\n",
      "Epoch 4/25\n",
      "52795/52795 [==============================] - 6s 110us/sample - loss: 0.0376 - val_loss: 0.0370\n",
      "Epoch 5/25\n",
      "52795/52795 [==============================] - 5s 101us/sample - loss: 0.0341 - val_loss: 0.0343\n",
      "Epoch 6/25\n",
      "52795/52795 [==============================] - 5s 102us/sample - loss: 0.0312 - val_loss: 0.0324\n",
      "Epoch 7/25\n",
      "52795/52795 [==============================] - 5s 104us/sample - loss: 0.0292 - val_loss: 0.0300\n",
      "Epoch 8/25\n",
      "52795/52795 [==============================] - 5s 99us/sample - loss: 0.0275 - val_loss: 0.0280\n",
      "Epoch 9/25\n",
      "52795/52795 [==============================] - 5s 100us/sample - loss: 0.0256 - val_loss: 0.0292\n",
      "Epoch 10/25\n",
      "52795/52795 [==============================] - 6s 107us/sample - loss: 0.0245 - val_loss: 0.0257\n",
      "Epoch 11/25\n",
      "52795/52795 [==============================] - 6s 104us/sample - loss: 0.0231 - val_loss: 0.0258\n",
      "Epoch 12/25\n",
      "52795/52795 [==============================] - 5s 102us/sample - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 13/25\n",
      "52795/52795 [==============================] - 6s 107us/sample - loss: 0.0215 - val_loss: 0.0232\n",
      "Epoch 14/25\n",
      "52795/52795 [==============================] - 5s 103us/sample - loss: 0.0206 - val_loss: 0.0223\n",
      "Epoch 15/25\n",
      "52795/52795 [==============================] - 5s 99us/sample - loss: 0.0202 - val_loss: 0.0229\n",
      "Epoch 16/25\n",
      "52795/52795 [==============================] - 5s 100us/sample - loss: 0.0194 - val_loss: 0.0218\n",
      "Epoch 17/25\n",
      "52795/52795 [==============================] - 6s 105us/sample - loss: 0.0189 - val_loss: 0.0255\n",
      "Epoch 18/25\n",
      "52795/52795 [==============================] - 6s 106us/sample - loss: 0.0187 - val_loss: 0.0260\n",
      "Epoch 19/25\n",
      "52795/52795 [==============================] - 5s 101us/sample - loss: 0.0186 - val_loss: 0.0200\n",
      "Epoch 20/25\n",
      "52795/52795 [==============================] - 6s 108us/sample - loss: 0.0179 - val_loss: 0.0210\n",
      "Epoch 21/25\n",
      "52795/52795 [==============================] - 5s 102us/sample - loss: 0.0173 - val_loss: 0.0192\n",
      "Epoch 22/25\n",
      "52795/52795 [==============================] - 6s 108us/sample - loss: 0.0173 - val_loss: 0.0204\n",
      "Epoch 23/25\n",
      "52795/52795 [==============================] - 5s 101us/sample - loss: 0.0169 - val_loss: 0.0191\n",
      "Epoch 24/25\n",
      "52795/52795 [==============================] - 5s 99us/sample - loss: 0.0159 - val_loss: 0.0188\n",
      "Epoch 25/25\n",
      "52795/52795 [==============================] - 6s 107us/sample - loss: 0.0162 - val_loss: 0.0174\n"
     ]
    }
   ],
   "source": [
    "model = MLPVariationalAutoEncoder(train_tensor_fl.shape[1], latent_dim, [1024, 512], final_activation='linear')\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(train_tensor_fl, train_tensor_fl, batch_size=512, epochs=25, verbose=1, shuffle=True,\n",
    "                    validation_data=(val_tensor_fl, val_tensor_fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random_idx = random.randrange(train_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Set) Input:  ['<s>', 'What', 'is', 'correct', 'way', 'to', 'a', 'remove', 'few', 'characters', 'at', 'the', 'end', 'of', 'string', 'C', '#', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Training Set) Input: \", tensor_to_tokenized_texts(np.array([train_tensor[random_idx]]), wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = np.reshape(model.predict(np.array([train_tensor_fl[random_idx]])), (1, train_tensor.shape[1], train_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Set) Reconstructed:  ['<s>', 'What', 'is', 'any', 'way', 'to', 'a', 'strings', 'reverse', 'characters', 'at', 'the', 'end', 'of', 'string', 'C', '#', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Training Set) Reconstructed: \", tensor_to_tokenized_texts(rec, wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('characters', 0.8964152336120605),\n",
       " ('numbers', 0.8511375188827515),\n",
       " ('regex', 0.8438798189163208),\n",
       " ('strings', 0.8184096813201904),\n",
       " ('words', 0.8077085614204407),\n",
       " ('char', 0.8045477867126465),\n",
       " ('Regex', 0.7984655499458313),\n",
       " ('comma', 0.7961603403091431),\n",
       " ('string', 0.7829160690307617),\n",
       " ('character', 0.777494490146637)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(rec[0, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_contents = open('../../data/iyer/test.txt').readlines()\n",
    "test = []\n",
    "for line in test_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        test.append(line.split('\\t')[2])\n",
    "test_tok = tokenize_texts(test)\n",
    "test_tensor = tokenized_texts_to_tensor(test_tok, wv, max_len)\n",
    "test_tensor_fl = np.reshape(test_tensor, (test_tensor.shape[0], test_tensor.shape[1] * test_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045086646639121125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_tensor_fl, test_tensor_fl, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random_idx = random.randrange(test_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Input:  ['<s>', 'detect', '<UNK>', 'when', 'copy', 'big', 'text', 'data', 'into', 'textbox', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Input: \", tensor_to_tokenized_texts(np.array([test_tensor[random_idx]]), wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Reconstructed:  ['<s>', 'know', 'hash', 'being', 'back', 'listing', 'text', 'data', 'into', 'textbox', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Reconstructed: \", tensor_to_tokenized_texts(np.reshape(model.predict(np.array([test_tensor_fl[random_idx]])), (1, test_tensor.shape[1], test_tensor.shape[2])), wv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
