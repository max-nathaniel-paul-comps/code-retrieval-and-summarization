{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"../../data/iyer/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_contents = train_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in train_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        train.append(line.split('\\t')[2] + \" ENDOFEXAMPLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52795"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = [word_tokenize(sample) for sample in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.Word2Vec(train_tok).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_tensors(wv, texts, max_len):\n",
    "    text_tensors = np.zeros((len(texts), max_len, wv.vector_size))\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(texts[i])):\n",
    "            if texts[i][j] in wv:\n",
    "                text_tensors[i][j] = wv[texts[i][j]]\n",
    "            else:\n",
    "                text_tensors[i][j] = np.ones((wv.vector_size,))\n",
    "    return text_tensors\n",
    "\n",
    "\n",
    "def tensors_to_texts(wv, tensors):\n",
    "    texts = []\n",
    "    for tensor in tensors:\n",
    "        text = \"\"\n",
    "        for i in range(0, len(tensor)):\n",
    "            if np.sum(tensor[i]) == wv.vector_size:\n",
    "                text += \"<UNK> \"\n",
    "            elif np.sum(tensor[i]) == 0:\n",
    "                break\n",
    "            else:\n",
    "                similar = wv.similar_by_vector(tensor[i])\n",
    "                if similar[0][0] == \"ENDOFEXAMPLE\":\n",
    "                    break\n",
    "                text += similar[0][0] + \" \"\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(text) for text in train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = texts_to_tensors(wv, train_tok, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C# getters, setters declaration ENDOFEXAMPLE'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = np.reshape(train_tensors, (train_tensors.shape[0], train_tensors.shape[1] * train_tensors.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52795, 3600)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_contents = open('../../data/iyer/valid.txt').readlines()\n",
    "val = []\n",
    "for line in val_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        val.append(line.split('\\t')[2])\n",
    "val_tok = [word_tokenize(sample) for sample in val]\n",
    "val_tensors = texts_to_tensors(wv, val_tok, max_len)\n",
    "val_tensor = np.reshape(val_tensors, (val_tensors.shape[0], val_tensors.shape[1] * val_tensors.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_vae import MLPVariationalAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52795 samples, validate on 6599 samples\n",
      "Epoch 1/12\n",
      "52795/52795 [==============================] - 8s 154us/sample - loss: 0.1105 - val_loss: 0.1083\n",
      "Epoch 2/12\n",
      "52795/52795 [==============================] - 6s 117us/sample - loss: 0.0866 - val_loss: 0.1029\n",
      "Epoch 3/12\n",
      "52795/52795 [==============================] - 6s 117us/sample - loss: 0.0797 - val_loss: 0.0968\n",
      "Epoch 4/12\n",
      "52795/52795 [==============================] - 6s 117us/sample - loss: 0.0749 - val_loss: 0.0877\n",
      "Epoch 5/12\n",
      "52795/52795 [==============================] - 6s 117us/sample - loss: 0.0727 - val_loss: 0.1072\n",
      "Epoch 6/12\n",
      "52795/52795 [==============================] - 6s 118us/sample - loss: 0.0710 - val_loss: 0.0826\n",
      "Epoch 7/12\n",
      "52795/52795 [==============================] - 6s 119us/sample - loss: 0.0700 - val_loss: 0.0863\n",
      "Epoch 8/12\n",
      "52795/52795 [==============================] - 6s 118us/sample - loss: 0.0687 - val_loss: 0.0926\n",
      "Epoch 9/12\n",
      "52795/52795 [==============================] - 6s 118us/sample - loss: 0.0682 - val_loss: 0.0913\n",
      "Epoch 10/12\n",
      "52795/52795 [==============================] - 6s 118us/sample - loss: 0.0674 - val_loss: 0.0829\n",
      "Epoch 11/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.0671 - val_loss: 0.0793\n",
      "Epoch 12/12\n",
      "52795/52795 [==============================] - 6s 119us/sample - loss: 0.0667 - val_loss: 0.0787\n"
     ]
    }
   ],
   "source": [
    "model = MLPVariationalAutoEncoder(train_tensor.shape[1], latent_dim, [1024, 512])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(train_tensor, train_tensor, batch_size=256, epochs=12, verbose=1, shuffle=True,\n",
    "                    validation_data=(val_tensor, val_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training) Input:  ['C # <UNK> , setters declaration ']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Training) Input: \", tensors_to_texts(wv, [train_tensors[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = np.reshape(model.predict(np.array([train_tensor[0]])), (1, train_tensors.shape[1], train_tensors.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training) Reconstructed:  ['C # Setter just Literal domain ']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Training) Reconstructed: \", tensors_to_texts(wv, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENDOFEXAMPLE', 0.7773289680480957),\n",
       " ('passing', 0.4660573899745941),\n",
       " ('Passing', 0.44540154933929443),\n",
       " ('Pass', 0.435727059841156),\n",
       " ('assigning', 0.4347077012062073),\n",
       " ('retaining', 0.43356168270111084),\n",
       " ('calling', 0.4285605549812317),\n",
       " ('built', 0.4266049265861511),\n",
       " ('specifying', 0.42437824606895447),\n",
       " ('javascript', 0.4207734167575836)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(rec[0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_contents = open('../../data/iyer/test.txt').readlines()\n",
    "test = []\n",
    "for line in test_file_contents:\n",
    "    items = line.split('\\t')\n",
    "    if len(items) == 5:\n",
    "        test.append(line.split('\\t')[2])\n",
    "test_tok = [word_tokenize(sample) for sample in test]\n",
    "test_tensors = texts_to_tensors(wv, test_tok, max_len)\n",
    "test_tensor = np.reshape(test_tensors, (test_tensors.shape[0], test_tensors.shape[1] * test_tensors.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10897430324124502"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_tensor, test_tensor, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_ex = test_tensor[random.randrange(test_tensor.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Input:  ['Use App Pool Credentials for WebClient Request ']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Input: \", tensors_to_texts(wv, np.reshape(random_test_ex, (1, test_tensors.shape[1], test_tensors.shape[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Reconstructed:  ['Delphi Code toolkit Nhibernate for VS2010 Setter controlling Assigning execution customized ui onclick crystal setter calling main keydown cmdlet i.e ']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Reconstructed: \", tensors_to_texts(wv, np.reshape(model.predict(np.array([random_test_ex])), (1, test_tensors.shape[1], test_tensors.shape[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52795 samples, validate on 6599 samples\n",
      "Epoch 1/12\n",
      "52795/52795 [==============================] - 7s 125us/sample - loss: 0.0660 - val_loss: 0.0998\n",
      "Epoch 2/12\n",
      "52795/52795 [==============================] - 6s 117us/sample - loss: 0.2774 - val_loss: 0.0975\n",
      "Epoch 3/12\n",
      "52795/52795 [==============================] - 6s 117us/sample - loss: 0.4154 - val_loss: 0.0932\n",
      "Epoch 4/12\n",
      "52795/52795 [==============================] - 6s 118us/sample - loss: 1.1977 - val_loss: 0.1452\n",
      "Epoch 5/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 1.1761 - val_loss: 0.3825\n",
      "Epoch 6/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.9504 - val_loss: 0.9789\n",
      "Epoch 7/12\n",
      "52795/52795 [==============================] - 6s 121us/sample - loss: 0.9051 - val_loss: 0.1106\n",
      "Epoch 8/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.3031 - val_loss: 0.0942\n",
      "Epoch 9/12\n",
      "52795/52795 [==============================] - 6s 121us/sample - loss: 0.1156 - val_loss: 0.0803\n",
      "Epoch 10/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.0847 - val_loss: 0.0809\n",
      "Epoch 11/12\n",
      "52795/52795 [==============================] - 6s 123us/sample - loss: 0.0792 - val_loss: 0.0748\n",
      "Epoch 12/12\n",
      "52795/52795 [==============================] - 6s 122us/sample - loss: 0.0748 - val_loss: 0.0733\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_tensor, train_tensor, batch_size=256, epochs=12, verbose=1, shuffle=True,\n",
    "                     validation_data=(val_tensor, val_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52795 samples, validate on 6599 samples\n",
      "Epoch 1/12\n",
      "52795/52795 [==============================] - 7s 126us/sample - loss: 0.0726 - val_loss: 0.0822\n",
      "Epoch 2/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 3/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.0715 - val_loss: 0.0739\n",
      "Epoch 4/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.0696 - val_loss: 0.0733\n",
      "Epoch 5/12\n",
      "52795/52795 [==============================] - 6s 123us/sample - loss: 0.0696 - val_loss: 0.0767\n",
      "Epoch 6/12\n",
      "52795/52795 [==============================] - 6s 122us/sample - loss: 0.0686 - val_loss: 0.0709\n",
      "Epoch 7/12\n",
      "52795/52795 [==============================] - 6s 122us/sample - loss: 0.0671 - val_loss: 0.0722\n",
      "Epoch 8/12\n",
      "52795/52795 [==============================] - 6s 122us/sample - loss: 0.0667 - val_loss: 0.0721\n",
      "Epoch 9/12\n",
      "52795/52795 [==============================] - 6s 120us/sample - loss: 0.0666 - val_loss: 0.0705\n",
      "Epoch 10/12\n",
      "52795/52795 [==============================] - 6s 119us/sample - loss: 0.0666 - val_loss: 0.0737\n",
      "Epoch 11/12\n",
      "52795/52795 [==============================] - 6s 119us/sample - loss: 0.0660 - val_loss: 0.0719\n",
      "Epoch 12/12\n",
      "52795/52795 [==============================] - 6s 119us/sample - loss: 0.0666 - val_loss: 0.0822\n"
     ]
    }
   ],
   "source": [
    "history3 = model.fit(train_tensor, train_tensor, batch_size=256, epochs=12, verbose=1, shuffle=True,\n",
    "                     validation_data=(val_tensor, val_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Input:  ['Use App Pool Credentials for WebClient Request ']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Input: \", tensors_to_texts(wv, np.reshape(random_test_ex, (1, test_tensors.shape[1], test_tensors.shape[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test Set) Reconstructed:  ['Details Silverlight Place Datasource for beep VS2010 ']\n"
     ]
    }
   ],
   "source": [
    "print(\"(Test Set) Reconstructed: \", tensors_to_texts(wv, np.reshape(model.predict(np.array([random_test_ex])), (1, test_tensors.shape[1], test_tensors.shape[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
